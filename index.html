<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Frederic Zhang - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="#top">Home</a></li>
					<li><a href="#work">Research</a></li>
					<li><a href="https://www.fredzzhang.com/photography">Photography</a></li>
				</ul>
			</nav>

		<!-- Home -->
			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-4 col-5-large col-12-medium">
							<span class="image fit"><img src="images/avatar.jpg" alt="Avatar" /></span>
							<div class="info">
								<h4 id="job">Postdoctoral Researcher</h4>
								<img style="vertical-align:middle" src="images/locator.png" height="50%">
								<a id="locator-link" href="https://maps.app.goo.gl/59kTadkcTGLcYTfy9">Adelaide, Australia</a> <br>
								<img src="images/email_adelaide.png" style="margin-top: 0.3em;" alt="Email" height=20px/>
							</div>
						</div>
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1> <strong>Frederic Zhang | 张真</strong> </h1>
							</header>
							<p>I'm currently a postdoctoral researcher at the Australian Institute for Machine Learning (AIML), working with <a href="https://ehsanabb.github.io/" target="_blank" rel="noopener noreferrer">Dr. Ehsan Abbasnejad</a>. </p>
								
							<p>I did my PhD at the Australian National University, under the supervision of <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Prof. Stephen Gould</a> and <a href="https://sites.google.com/view/djcampbell/" target="_blank", rel="noopener noreferrer">Dr. Dylan Campbell</a>. My research focus was on the visual and spatial understanding of human&ndash;object interactions. This includes visual recognition and localisation.</p>
							
							<p>Prior to my PhD, as part of an international partnership program, I received a bachelor degree of science in automation from Beijing Institute of Technology and a bachelor degree of engineering in mechatronics (research and development) with first-class honours from the Australian National University, where I had the pleasure to work with <a href="https://scholar.google.com.au/citations?user=fddAbqsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Prof. Yuchao Dai</a> and <a href="https://researchers.anu.edu.au/researchers/hartley-r" target="_blank" rel="noopener noreferrer">Prof. Richard Hartley</a>. </p>
							
							<p>I'm greatly passionate about programming, so much so that I wrote a deep learning library called <a href="https://github.com/fredzzhang/pocket">Pocket</a>. It is a lightweight library built on top of PyTorch, featuring different boilerplate learning engines and other utilities purposed for visualisation and evaluation. I'm also a photographer. As an enthusiast of the great outdoors, my subjects are mostly nature-oriented. Find out more in <a href="https://www.fredzzhang.com/photography">my gallery</a>!</p> 
							<hr>
							<h3>Connect with me</h3>
							<ul class="social">
								<a href="https://github.com/fredzzhang"><span class="social-icon"><img class="icon" alt="Github" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg"></span></a>

								<a href="https://scholar.google.com/citations?user=b1PQadgAAAAJ&hl=en"><span class="social-icon"><img class="icon" alt="Google Scholar" src="https://user-images.githubusercontent.com/11484831/124265883-aaeb3880-db79-11eb-98c0-982548e03488.png"></span></a>

								<a href="https://www.linkedin.com/in/frederic-z-zhang"><span class="social-icon"><img class="icon" alt="LinkedIn" src="https://user-images.githubusercontent.com/11484831/102576050-c72d0800-4148-11eb-96bb-8c634bdfcb05.png"></span></a>

								<a href="https://twitter.com/fredzzhang"><span class="social-icon"><img class="icon" alt="Twitter" src="https://user-images.githubusercontent.com/11484831/102576039-bc727300-4148-11eb-9b25-1d5dc283223d.png"></span></a>

								<a href="https://www.youtube.com/channel/UCTtycgodYPRS6xtZsxJzdug"><span class="social-icon"><img class="icon" alt="Youtube" src="https://user-images.githubusercontent.com/11484831/102576019-afee1a80-4148-11eb-9dee-0efd9a6b98ba.png"></span></a>
								
								<a href="https://www.instagram.com/fredzzhang/"><span class="social-icon"><img class="icon" alt="Instagram" src="https://user-images.githubusercontent.com/11484831/102576056-d01dd980-4148-11eb-941b-7fba9b89feeb.png"></span></a>
								
							</ul>
						</div>
					</div>
				</div>
			</article>

		<!-- Work -->
			<article id="work" class="wrapper style2">
				<div class="container">
					<header>
						<h2 style="margin-bottom: 2em;">Research</h2>
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://github.com/fredzzhang/pvic"><img src="images/publications/pvic.png"></a></div>
							<div class="paper-info">
									Exploring Predicate Visual Context in Detecting Human&ndash;Object Interactions
									<span id="paper-author">
										<b>Frederic Z. Zhang</b>,
										<a href="https://github.com/PkuRainBow" target="_blank" rel="noopener noreferrer">Yuhui Yuan</a>, 
										<a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a>, 
										<a href="https://zhongzhuoyao.github.io/" target="_blank" rel="noopener noreferrer">Zhuoyao Zhong</a> 
										and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a>
									</span>
									<span id="paper-venue">In Proceedings of the <i>IEEE International Conference on Computer Vision (ICCV)</i>, 2023.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('upt-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2308.06202.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/pvic">code</a>]
										<!-- [<a href="https://www.youtube.com/watch?v=VneKesTm_gw">video</a>] -->
										[<a href="javascript:void(0);" onclick="showTextBox('upt-bibtex')">bibtex</a>]
									</span>
									<div id="upt-abstract" class="paper-abstract">
										Recently, the DETR framework has emerged as the dominant approach for human&ndash;object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
									</div>
									<div id="upt-bibtex", class="paper-bibtex">
										@inproceedings{zhang2022upt,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Campbell, Dylan and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Exploring Predicate Visual Context in Detecting Human&ndash;Object Interactions}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, <br>
										&nbsp;&nbsp;month = {October}, <br>
										&nbsp;&nbsp;year = {2023}, <br>
										&nbsp;&nbsp;pages = {10411-10421}, <br>
										}
									</div>
							</div>
						</div>

						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://fredzzhang.com/unary-pairwise-transformers/"><img src="images/publications/tokens.png"></a></div>
							<div class="paper-info">
									Efficient Two-Stage Detection of Human&ndash;Object Interactions with a Novel Unary&ndash;Pairwise Transformer
									<span id="paper-author"><b>Frederic Z. Zhang</b>, <a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a> and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a></span>
									<span id="paper-venue">In Proceedings of the <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</i> 2022.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('upt-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Efficient_Two-Stage_Detection_of_Human-Object_Interactions_With_a_Novel_Unary-Pairwise_CVPR_2022_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2112.01838.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/upt">code</a>]
										[<a href="https://www.youtube.com/watch?v=VneKesTm_gw">video</a>]
										[<a href="javascript:void(0);" onclick="showTextBox('upt-bibtex')">bibtex</a>]
									</span>
									<div id="upt-abstract" class="paper-abstract">
										Recent developments in transformer models for visual data have led to significant improvements in recognition and detection tasks. In particular, using learnable queries in place of region proposals has given rise to a new class of one-stage detection models, spearheaded by the Detection Transformer (DETR). Variations on this one-stage approach have since dominated human&ndash;object interaction (HOI) detection. However, the success of such one-stage HOI detectors can largely be attributed to the representation power of transformers. We discovered that when equipped with the same transformer, their two-stage counterparts can be more performant and memory-efficient, while taking a fraction of the time to train. In this work, we propose the Unary&ndash;Pairwise Transformer, a two-stage detector that exploits unary and pairwise representations for HOIs. We observe that the unary and pairwise parts of our transformer network specialize, with the former preferentially increasing the scores of positive examples and the latter decreasing the scores of negative examples. We evaluate our method on the HICO-DET and V-COCO datasets, and significantly outperform state-of-the-art approaches. At inference time, our model with ResNet50 approaches real-time performance on a single GPU.
									</div>
									<div id="upt-bibtex", class="paper-bibtex">
										@inproceedings{zhang2022upt,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Campbell, Dylan and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Efficient Two-Stage Detection of Human&ndash;Object Interactions with a Novel Unary&ndash;Pairwise Transformer}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
										&nbsp;&nbsp;month = {June}, <br>
										&nbsp;&nbsp;year = {2022}, <br>
										&nbsp;&nbsp;pages = {20104-20112} <br>
										}
									</div>
							</div>
						</div>
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://github.com/fredzzhang/spatially-conditioned-graphs"><img src="images/publications/scg.png"></a></div>
							<div class="paper-info">
									Spatially Conditioned Graphs for Detecting Human&ndash;Object Interactions
									<span id="paper-author"><b>Frederic Z. Zhang</b>, <a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a> and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a></span>
									<span id="paper-venue">In Proceedings of the <i>IEEE International Conference on Computer Vision (ICCV)</i>, 2021.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('scg-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Spatially_Conditioned_Graphs_for_Detecting_Human-Object_Interactions_ICCV_2021_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2012.06060.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/spatially-conditioned-graphs">code</a>]
										[<a href="https://www.youtube.com/watch?v=gkBWi_rWedU">video</a>]
										[<a href="javascript:void(0);" onclick="showTextBox('scg-bibtex')">bibtex</a>]
									</span>
									<div id="scg-abstract" class="paper-abstract">
										We address the problem of detecting human&ndash;object interactions in images using graphical neural networks. Unlike conventional methods, where nodes send scaled but otherwise identical messages to each of their neighbours, we propose to condition messages between pairs of nodes on their spatial relationships, resulting in different messages going to neighbours of the same node. To this end, we explore various ways of applying spatial conditioning under a multi-branch structure. Through extensive experimentation we demonstrate the advantages of spatial conditioning for the computation of the adjacency structure, messages and the refined graph features. In particular, we empirically show that as the quality of the bounding boxes increases, their coarse appearance features contribute relatively less to the disambiguation of interactions compared to the spatial information. Our method achieves an mAP of 31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming state of the art on fine-tuned detections.
									</div>
									<div id="scg-bibtex", class="paper-bibtex">
										@inproceedings{zhang2021scg,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Campbell, Dylan and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Spatially Conditioned Graphs for Detecting Human&ndash;Object Interactions}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, <br>
										&nbsp;&nbsp;month = {October}, <br>
										&nbsp;&nbsp;year = {2021}, <br>
										&nbsp;&nbsp;pages = {13319-13327} <br>
										}
									</div>
							</div>
						</div>
				</div>
			</article>

			<div class="footer">
				&copy; 2021 Frederic Zhang
			</div>

		<!-- Scripts -->
			<script src="assets/js/textbox.js"></script>
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
