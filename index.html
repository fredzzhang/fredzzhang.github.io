<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Frederic Zhang - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="#top">Home</a></li>
					<li><a href="#work">Research</a></li>
					<li><a href="https://www.fredzzhang.com/photography">Photography</a></li>
				</ul>
			</nav>

		<!-- Home -->
			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-4 col-5-large col-12-medium">
							<span class="image fit"><img src="images/avatar_2024.jpg" alt="Avatar" /></span>
							<div class="info">
								<h4 id="job">Research Fellow</h4>
								<img style="vertical-align:middle" src="images/locator.png" height="50%">
								<a id="locator-link" href="https://maps.app.goo.gl/59kTadkcTGLcYTfy9">Adelaide, Australia</a> <br>
								<img src="images/email_adelaide.png" style="margin-top: 0.3em;" alt="Email" height=20px/>
							</div>
						</div>
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1> <strong>Frederic Zhang | 张真</strong> </h1>
							</header>
							<p>I'm currently a research fellow at the Centre for Augmented Reasoning, Australian Institute for Machine Learning (AIML), working with <a href="https://ehsanabb.github.io/" target="_blank" rel="noopener noreferrer">Dr. Ehsan Abbasnejad</a>. </p>
								
							<p>I did my PhD at the Australian National University, under the supervision of <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Prof. Stephen Gould</a> and <a href="https://sites.google.com/view/djcampbell/" target="_blank", rel="noopener noreferrer">Dr. Dylan Campbell</a>. My research focus was on the visual and spatial understanding of human&ndash;object interactions. This includes visual recognition and localisation.</p>
							
							<p>Prior to my PhD, as part of an international partnership program, I received a bachelor degree of science in automation from Beijing Institute of Technology and a bachelor degree of engineering in mechatronics (research and development) with first-class honours from the Australian National University, where I had the pleasure to work with <a href="https://scholar.google.com.au/citations?user=fddAbqsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Prof. Yuchao Dai</a> and <a href="https://researchers.anu.edu.au/researchers/hartley-r" target="_blank" rel="noopener noreferrer">Prof. Richard Hartley</a>. </p>
							
							<p>I'm greatly passionate about programming, so much so that I wrote a deep learning library called <a href="https://github.com/fredzzhang/pocket">Pocket</a>. It is a lightweight library built on top of PyTorch, featuring different boilerplate learning engines and other utilities purposed for visualisation and evaluation. I'm also a photographer. As an enthusiast of the great outdoors, my subjects are mostly nature-oriented. Find out more in <a href="https://www.fredzzhang.com/photography">my gallery</a>!</p> 
							<hr>
							<h3>Connect with me</h3>
							<ul class="social">
								<a href="https://github.com/fredzzhang"><span class="social-icon"><img class="icon" alt="Github" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg"></span></a>

								<a href="https://scholar.google.com/citations?user=b1PQadgAAAAJ&hl=en"><span class="social-icon"><img class="icon" alt="Google Scholar" src="https://user-images.githubusercontent.com/11484831/124265883-aaeb3880-db79-11eb-98c0-982548e03488.png"></span></a>

								<a href="https://www.linkedin.com/in/frederic-z-zhang"><span class="social-icon"><img class="icon" alt="LinkedIn" src="https://user-images.githubusercontent.com/11484831/102576050-c72d0800-4148-11eb-96bb-8c634bdfcb05.png"></span></a>

								<a href="https://twitter.com/fredzzhang"><span class="social-icon"><img class="icon" alt="Twitter" src="https://user-images.githubusercontent.com/11484831/102576039-bc727300-4148-11eb-9b25-1d5dc283223d.png"></span></a>

								<a href="https://www.youtube.com/channel/UCTtycgodYPRS6xtZsxJzdug"><span class="social-icon"><img class="icon" alt="Youtube" src="https://user-images.githubusercontent.com/11484831/102576019-afee1a80-4148-11eb-9dee-0efd9a6b98ba.png"></span></a>
								
								<a href="https://www.instagram.com/fredzzhang/"><span class="social-icon"><img class="icon" alt="Instagram" src="https://user-images.githubusercontent.com/11484831/102576056-d01dd980-4148-11eb-941b-7fba9b89feeb.png"></span></a>
								
							</ul>
						</div>
					</div>
				</div>
			</article>

		<!-- Work -->
			<article id="work" class="wrapper style2">
				<div class="container">
					<header>
						<h2 style="margin-bottom: 2em;">Research</h2>
						<!-- aTLAS -->
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://github.com/fredzzhang/atlas"><img src="images/publications/atlas.png"></a></div>
							<div class="paper-info">
									Knowledge Composition using Task Vectors with Learned Anisotropic Scaling
									<span id="paper-author">
										<b>Frederic Z. Zhang*</b>,
										<a href="https://scholar.google.com/citations?user=trUhFBEAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Paul Albert*</a>, 
										<a href="https://crodriguezo.me/" target="_blank" rel="noopener noreferrer">Cristian Rodriguez-Opazo</a>, 
										<a href="https://scholar.google.com.au/citations?user=nMGZ2ZQAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Anton ven den Hengel</a> 
										and <a href="https://ehsanabb.github.io/" target="_blank" rel="noopener noreferrer">Ehsan Abbasnejad</a>
									</span>
									<span id="paper-venue">arXiv preprint, 2024.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('atlas-abstract')">abstract</a>]
										<!-- [<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.html">paper</a>] -->
										[<a href="https://arxiv.org/pdf/2407.02880">preprint</a>]
										[<a href="https://github.com/fredzzhang/atlas">code</a>]
										<!-- [<a href="https://www.youtube.com/watch?v=iYqOAXJFqpU">video</a>] -->
										[<a href="javascript:void(0);" onclick="showTextBox('atlas-bibtex')">bibtex</a>]
									</span>
									<div id="atlas-abstract" class="paper-abstract">
										Pre-trained models produce strong generic representations that can be adapted via fine-tuning. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labeled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a PEFT method, particularly with less data, and demonstrate that its scalibility.
									</div>
									<div id="atlas-bibtex", class="paper-bibtex">
										@misc{zhang2024atlas,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Albert, Paul and Rodriguez-Opazo, Cristian and van den Hengel, Anton and Abbasnejad, Ehsan}, <br>
										&nbsp;&nbsp;title = {Knowledge Composition using Task Vectors with Learned Anisotropic Scaling}, <br>
										&nbsp;&nbsp;eprint={2407.02880}, <br>
										&nbsp;&nbsp;archivePrefix={arXiv}, <br>
										&nbsp;&nbsp;primaryClass={cs.LG}, <br>
										&nbsp;&nbsp;url={https://arxiv.org/abs/2407.02880}, <br>
										&nbsp;&nbsp;year = {2024}, <br>
										}
									</div>
							</div>
						</div>
						<!-- Thesis 2024-->
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://openresearch-repository.anu.edu.au/items/2f2331b2-77d4-422a-8acd-093a8d894895"><img src="images/publications/anu_logo.png"></a></div>
							<div class="paper-info">
									Visual and Spatial Understanding of Human&ndash;Object Interactions
									<span id="paper-author">
										<b>Frederic Z. Zhang</b>
									</span>
									<span id="paper-venue">PhD Thesis, 2024.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('thesis-abstract')">abstract</a>]
										[<a href="https://openresearch-repository.anu.edu.au/items/2f2331b2-77d4-422a-8acd-093a8d894895">thesis</a>]
										<!-- [<a href="https://www.youtube.com/watch?v=iYqOAXJFqpU">oral presentation</a>] -->
										[<a href="javascript:void(0);" onclick="showTextBox('thesis-bibtex')">bibtex</a>]
									</span>
									<div id="thesis-abstract" class="paper-abstract">
										In the context of computer vision, human-object interactions (HOIs) are often characterised as a (subject, predicate, object) triplet, with humans being the subject. As such, to understand HOIs is to localise pairs of interactive instances and recognise the predicates that signify their interactions.<br/><br/>
										This interpretation naturally leads to a graph structure, where humans and objects are represented as nodes while their interactions as edges. We investigate this idea by employing off-the-shelf object detectors to obtain a set of human and object detections, and building a bipartite graph with human nodes on one side of the bipartition and object nodes on the other. Unlike conventional methods, wherein nodes send scaled but otherwise identical messages to each of their neighbours, we propose to condition the message passing between pairs of nodes on their spatial relationships. With spatial conditioning, the proposed method is able to suppress numerous negative pairs due to incompatible spatial relationships, and particularly excels at inferring the correspondence between interactive humans and objects when there are many pairs in the same scene. In addition, we observe that the learned adjacency matrices spontaneously exhibit structures indicative of interactive pairs without explicit supervision.<br/><br/>
										Such emergent properties prompt us to investigate further into the formulation of graphs. Apart from the unary representations (human and object instances), we incorporate human&ndash;object pairs into the graph structure by encoding each pair as a node. Utilising the popular transformer architecture, we propose the unary--pairwise transformer, wherein self-attention blocks serve as fully-connected graphs. We observe that when separate self-attention blocks are employed for the unary and pairwise representations, they specialise in complimentary ways.
										Specifically, the unary layer preferentially increases the scores of positive human&ndash;object pairs, while the pairwise layer decreases the scores of negative pairs.<br/><br/>
										Despite the success in graphical modelling of HOIs, their complexity and ambiguity still poses a challenge. Through extensive visualisations, we observe that the commonly used object features are often extracted from object extremities, thus lacking the fine-grained contexts to disambiguate certain interactions. In particular, we identify two types of visual contexts lacking in current feature formulations and propose to enrich the representations with spatially-guided cross-attention, where a carefully designed box-pair positional embeddings serve as spatial biases.
										With rich visualisations, we demonstrate how the spatial guidance impacts the attention mechanism and supplements the model with key visual cues for the recognition of HOIs.
									</div>
									<div id="thesis-bibtex", class="paper-bibtex">
										@phdthesis{zhang2024thesis,<br>
										&nbsp;&nbsp;title        = {Visual and Spatial Understanding of Human&ndash;Object Interactions}, <br>
										&nbsp;&nbsp;author       = {Frederic Z. Zhang}, <br>
										&nbsp;&nbsp;year         = {2024}, <br>
										&nbsp;&nbsp;month        = {Mar}, <br>
										&nbsp;&nbsp;address      = {Canberra, Australia}, <br>
										&nbsp;&nbsp;note         = {Available at \url{https://openresearch-repository.anu.edu.au/items/2f2331b2-77d4-422a-8acd-093a8d894895}}, <br>
										&nbsp;&nbsp;school       = {College of engineering, computing and cybernatics, The Australian National University}, <br>
										&nbsp;&nbsp;type         = {PhD thesis} <br>
										}
									</div>
							</div>
						</div>
						<!-- PViC ICCV'23 -->
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://github.com/fredzzhang/pvic"><img src="images/publications/pvic.png"></a></div>
							<div class="paper-info">
									Exploring Predicate Visual Context in Detecting Human&ndash;Object Interactions
									<span id="paper-author">
										<b>Frederic Z. Zhang</b>,
										<a href="https://github.com/PkuRainBow" target="_blank" rel="noopener noreferrer">Yuhui Yuan</a>, 
										<a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a>, 
										<a href="https://zhongzhuoyao.github.io/" target="_blank" rel="noopener noreferrer">Zhuoyao Zhong</a> 
										and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a>
									</span>
									<span id="paper-venue">International Conference on Computer Vision (ICCV), 2023.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('pvic-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2308.06202.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/pvic">code</a>]
										[<a href="https://www.youtube.com/watch?v=iYqOAXJFqpU">video</a>]
										[<a href="javascript:void(0);" onclick="showTextBox('pvic-bibtex')">bibtex</a>]
									</span>
									<div id="pvic-abstract" class="paper-abstract">
										Recently, the DETR framework has emerged as the dominant approach for human&ndash;object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
									</div>
									<div id="pvic-bibtex", class="paper-bibtex">
										@inproceedings{zhang2023pvic,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Yuan, Yuhui and Campbell, Dylan and Zhong, Zhuoyao and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Exploring Predicate Visual Context in Detecting Human&ndash;Object Interactions}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, <br>
										&nbsp;&nbsp;month = {October}, <br>
										&nbsp;&nbsp;year = {2023}, <br>
										&nbsp;&nbsp;pages = {10411-10421}, <br>
										}
									</div>
							</div>
						</div>
						<!-- UPT CVPR'22 -->
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://fredzzhang.com/unary-pairwise-transformers/"><img src="images/publications/tokens.png"></a></div>
							<div class="paper-info">
									Efficient Two-Stage Detection of Human&ndash;Object Interactions with a Novel Unary&ndash;Pairwise Transformer
									<span id="paper-author"><b>Frederic Z. Zhang</b>, <a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a> and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a></span>
									<span id="paper-venue">Computer Vision and Pattern Recognition (CVPR), 2022.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('upt-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Efficient_Two-Stage_Detection_of_Human-Object_Interactions_With_a_Novel_Unary-Pairwise_CVPR_2022_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2112.01838.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/upt">code</a>]
										[<a href="https://www.youtube.com/watch?v=VneKesTm_gw">video</a>]
										[<a href="javascript:void(0);" onclick="showTextBox('upt-bibtex')">bibtex</a>]
									</span>
									<div id="upt-abstract" class="paper-abstract">
										Recent developments in transformer models for visual data have led to significant improvements in recognition and detection tasks. In particular, using learnable queries in place of region proposals has given rise to a new class of one-stage detection models, spearheaded by the Detection Transformer (DETR). Variations on this one-stage approach have since dominated human&ndash;object interaction (HOI) detection. However, the success of such one-stage HOI detectors can largely be attributed to the representation power of transformers. We discovered that when equipped with the same transformer, their two-stage counterparts can be more performant and memory-efficient, while taking a fraction of the time to train. In this work, we propose the Unary&ndash;Pairwise Transformer, a two-stage detector that exploits unary and pairwise representations for HOIs. We observe that the unary and pairwise parts of our transformer network specialize, with the former preferentially increasing the scores of positive examples and the latter decreasing the scores of negative examples. We evaluate our method on the HICO-DET and V-COCO datasets, and significantly outperform state-of-the-art approaches. At inference time, our model with ResNet50 approaches real-time performance on a single GPU.
									</div>
									<div id="upt-bibtex", class="paper-bibtex">
										@inproceedings{zhang2022upt,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Campbell, Dylan and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Efficient Two-Stage Detection of Human&ndash;Object Interactions with a Novel Unary&ndash;Pairwise Transformer}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
										&nbsp;&nbsp;month = {June}, <br>
										&nbsp;&nbsp;year = {2022}, <br>
										&nbsp;&nbsp;pages = {20104-20112} <br>
										}
									</div>
							</div>
						</div>
						<!-- SCG ICCV'21 -->
						<div class="paper gradient-shadow">
							<div class="paper-teaser"><a href="https://github.com/fredzzhang/spatially-conditioned-graphs"><img src="images/publications/scg.png"></a></div>
							<div class="paper-info">
									Spatially Conditioned Graphs for Detecting Human&ndash;Object Interactions
									<span id="paper-author"><b>Frederic Z. Zhang</b>, <a href="https://sites.google.com/view/djcampbell/" target="_blank" rel="noopener noreferrer">Dylan Campbell</a> and <a href="http://users.cecs.anu.edu.au/~sgould/" target="_blank" rel="noopener noreferrer">Stephen Gould</a></span>
									<span id="paper-venue">International Conference on Computer Vision (ICCV), 2021.</span>
									<span id="paper-links">
										[<a href="javascript:void(0);" onclick="showTextBox('scg-abstract')">abstract</a>]
										[<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Spatially_Conditioned_Graphs_for_Detecting_Human-Object_Interactions_ICCV_2021_paper.html">paper</a>]
										[<a href="https://arxiv.org/pdf/2012.06060.pdf">preprint</a>]
										[<a href="https://github.com/fredzzhang/spatially-conditioned-graphs">code</a>]
										[<a href="https://www.youtube.com/watch?v=gkBWi_rWedU">video</a>]
										[<a href="javascript:void(0);" onclick="showTextBox('scg-bibtex')">bibtex</a>]
									</span>
									<div id="scg-abstract" class="paper-abstract">
										We address the problem of detecting human&ndash;object interactions in images using graphical neural networks. Unlike conventional methods, where nodes send scaled but otherwise identical messages to each of their neighbours, we propose to condition messages between pairs of nodes on their spatial relationships, resulting in different messages going to neighbours of the same node. To this end, we explore various ways of applying spatial conditioning under a multi-branch structure. Through extensive experimentation we demonstrate the advantages of spatial conditioning for the computation of the adjacency structure, messages and the refined graph features. In particular, we empirically show that as the quality of the bounding boxes increases, their coarse appearance features contribute relatively less to the disambiguation of interactions compared to the spatial information. Our method achieves an mAP of 31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming state of the art on fine-tuned detections.
									</div>
									<div id="scg-bibtex", class="paper-bibtex">
										@inproceedings{zhang2021scg,<br>
										&nbsp;&nbsp;author = {Zhang, Frederic Z. and Campbell, Dylan and Gould, Stephen}, <br>
										&nbsp;&nbsp;title = {Spatially Conditioned Graphs for Detecting Human&ndash;Object Interactions}, <br>
										&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, <br>
										&nbsp;&nbsp;month = {October}, <br>
										&nbsp;&nbsp;year = {2021}, <br>
										&nbsp;&nbsp;pages = {13319-13327} <br>
										}
									</div>
							</div>
						</div>
				</div>
			</article>

			<div class="footer">
				&copy; 2021 Frederic Zhang
			</div>

		<!-- Scripts -->
			<script src="assets/js/textbox.js"></script>
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
